{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import bamboolib as bam\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9128ca",
   "metadata": {},
   "source": [
    "## Segédfüggvények"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdata_to_tdata_split(parquet_data,train_percent,val_percent):\n",
    "\n",
    "  if train_percent + val_percent > 100:\n",
    "    print(\"Sum of Train and Val must be less than 100\\n\")\n",
    "    return\n",
    "    \n",
    "  d_parquet_len = len(parquet_data)\n",
    "\n",
    "  first_key = parquet_data.index.start\n",
    "  str_date = str(parquet_data.tpep_pickup_datetime[first_key])\n",
    "  int_date = int(str_date[0:4])\n",
    "\n",
    "  train_size = int(train_percent/100*d_parquet_len)\n",
    "  val_size = int(val_percent/100*d_parquet_len)\n",
    "\n",
    "  TrainX = torch.rand(train_size,7)\n",
    "  TrainY = torch.rand(train_size,2)\n",
    "\n",
    "  ValX = torch.rand(val_size,7)\n",
    "  ValY = torch.rand(val_size,2)\n",
    "\n",
    "  if int_date == 2019:\n",
    "    valy = torch.tensor([1,0],dtype=torch.float)\n",
    "  if int_date == 2020:\n",
    "    valy = torch.tensor([0,1],dtype=torch.float)\n",
    "\n",
    "  for i in range(train_size):\n",
    "    TrainX[i,0:7] = torch.tensor([parquet_data.trip_distance[i],parquet_data.passenger_count[i],parquet_data.payment_type[i] + 1,\n",
    "                        parquet_data.fare_amount[i],parquet_data.extra[i],parquet_data.tip_amount[i],parquet_data.improvement_surcharge[i]],dtype=torch.float)\n",
    "\n",
    "    TrainY[i,0:2] = valy\n",
    "\n",
    "  for i in range(val_size):\n",
    "    ValX[i,0:7] = torch.tensor([parquet_data.trip_distance[i+train_size],parquet_data.passenger_count[i+train_size],parquet_data.payment_type[i+train_size] + 1,\n",
    "                        parquet_data.fare_amount[i+train_size],parquet_data.extra[i+train_size],parquet_data.tip_amount[i+train_size],parquet_data.improvement_surcharge[i+train_size]],dtype=torch.float)\n",
    "  \n",
    "    ValY[i,0:2] = valy\n",
    "\n",
    "  return TrainX,TrainY,  ValX,ValY\n",
    "\n",
    "\n",
    "def make_dataloaders(l_parquet_datas,batch_size,train_percent=80,val_percent=20):\n",
    "\n",
    "  whole_train_size = 0\n",
    "  whole_val_size = 0\n",
    "\n",
    "  for data in l_parquet_datas:\n",
    "    d_parquet_len = len(data)\n",
    "    d_train_size = int(train_percent/100*d_parquet_len)\n",
    "    d_val_size = int(val_percent/100*d_parquet_len)\n",
    "\n",
    "    whole_train_size += d_train_size\n",
    "    whole_val_size += d_val_size\n",
    "  \n",
    "  TrainX = torch.rand(whole_train_size,7)\n",
    "  TrainY = torch.rand(whole_train_size,2)\n",
    "\n",
    "  ValX = torch.rand(whole_val_size,7)\n",
    "  ValY = torch.rand(whole_val_size,2)\n",
    "\n",
    "  i = 0\n",
    "  j = 0\n",
    "\n",
    "  for data in l_parquet_datas:\n",
    "\n",
    "    d_parquet_len = len(data)\n",
    "    d_train_size = int(train_percent/100*d_parquet_len)\n",
    "    d_val_size = int(val_percent/100*d_parquet_len)\n",
    "\n",
    "    TrainX1,TrainY1,ValX1,ValY1 = pdata_to_tdata_split(data,train_percent,val_percent)\n",
    "\n",
    "    TrainX[i:i+d_train_size,0:7] = TrainX1\n",
    "    TrainY[i:i+d_train_size,0:2] = TrainY1\n",
    "\n",
    "    ValX[j:j+d_val_size,0:7] = ValX1\n",
    "    ValY[j:j+d_val_size,0:2] = ValY1\n",
    "\n",
    "    i += d_train_size\n",
    "    j += d_val_size\n",
    "\n",
    "  train_dataset = TensorDataset(TrainX,TrainY)\n",
    "  train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "  val_dataset = TensorDataset(ValX,ValY)\n",
    "  val_dataloader = DataLoader(val_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "  print(\"Done!\")\n",
    "  \n",
    "  return train_dataloader,val_dataloader\n",
    "\n",
    "\n",
    "def make_val_dataloader(l_parquet_datas):\n",
    "  whole_test_size = 0\n",
    "\n",
    "  for parquet_data in l_parquet_datas:\n",
    "    size = len(parquet_data)\n",
    "    whole_test_size += size\n",
    "  \n",
    "  TestX = torch.rand(whole_test_size,7)\n",
    "  TestY = torch.rand(whole_test_size,2)\n",
    "\n",
    "  k = 0\n",
    "  for parquet_data in l_parquet_datas:\n",
    "    size = len(parquet_data)\n",
    "\n",
    "    first_key = parquet_data.index.start\n",
    "    str_date = str(parquet_data.tpep_pickup_datetime[first_key])\n",
    "    int_date = int(str_date[0:4])\n",
    "\n",
    "    if int_date == 2019:\n",
    "      testy = torch.tensor([1,0],dtype=torch.float)\n",
    "    if int_date == 2020:\n",
    "      testy = torch.tensor([0,1],dtype=torch.float)\n",
    "\n",
    "    TestX1 = torch.rand(size,7)\n",
    "    TestY1 = torch.rand(size,2)\n",
    "\n",
    "    j = first_key\n",
    "    for i in range(size):\n",
    "      TestX1[i,0:7] = torch.tensor([parquet_data.trip_distance[i+j],parquet_data.passenger_count[i+j],parquet_data.payment_type[i+j] + 1,\n",
    "                        parquet_data.fare_amount[i+j],parquet_data.extra[i+j],parquet_data.tip_amount[i+j],parquet_data.improvement_surcharge[i+j]],dtype=torch.float)\n",
    "\n",
    "      TestY1[i,0:2] = testy\n",
    "    \n",
    "\n",
    "    TestX[k:k+size,0:7] = TestX1\n",
    "    TestY[k:k+size,0:2] = TestY1\n",
    "\n",
    "    k += size\n",
    "\n",
    "  test_dataset = TensorDataset(TestX,TestY)\n",
    "  test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "  print(\"Done!\")\n",
    "  \n",
    "  return test_dataloader\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loss_fn,dataloader,optimizer,scheduler):\n",
    "\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  correct = 0\n",
    "  running_loss = 0\n",
    "  bestLoss = 1000\n",
    "\n",
    "  for X,y in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      y_pred = model(X)\n",
    "      correct += (y_pred.argmax() == y.argmax()).type(torch.float).sum().item()\n",
    "      loss = loss_fn(y_pred,y)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "  \n",
    "  scheduler.step()\n",
    "\n",
    "  running_loss /= len(dataloader)\n",
    "  correct /= size\n",
    "\n",
    "  print(f'Loss: {running_loss}')\n",
    "  if running_loss < bestLoss:\n",
    "    bestLoss = running_loss\n",
    "    torch.save(model.state_dict(),\"myNet.pth\")\n",
    "    \n",
    "\n",
    "def test_loop(model,dataloader,percent):\n",
    "  size = percent/100*len(dataloader.dataset)\n",
    "  correct = 0\n",
    "\n",
    "  i = 0\n",
    "  with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax() == y.argmax()).type(torch.float).sum().item()\n",
    "\n",
    "            i += 1\n",
    "            if i >= size:\n",
    "              break\n",
    "\n",
    "  correct /= size\n",
    "\n",
    "  print(f\"Test Accuracy: {(100*correct):>0.2f}%\\n\")\n",
    "\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ccf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myOptimizers(model,lr,idx):\n",
    "    m = idx % 3\n",
    "    if m == 0:\n",
    "        return optim.Adam(model.parameters(),lr=lr,weight_decay=1e-4)\n",
    "    if m == 1:\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "    if m == 2:\n",
    "        return torch.optim.Rprop(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def myLossFunctions(rnd):\n",
    "    if rnd == 0:\n",
    "        return nn.CrossEntropyLoss()\n",
    "    if rnd == 1:\n",
    "        return nn.MSELoss()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d345cf",
   "metadata": {},
   "source": [
    "## Adatok importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742cc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2019_01_file = r'C:\\Users\\User\\Desktop\\parquet_files\\yellow_tripdata_2019-01.parquet'\n",
    "p2019_02_file = r'C:\\Users\\User\\Desktop\\parquet_files\\yellow_tripdata_2019-02.parquet'\n",
    "\n",
    "\n",
    "p2020_03_file = r'C:\\Users\\User\\Desktop\\parquet_files\\yellow_tripdata_2020-03.parquet'\n",
    "p2020_04_file = r'C:\\Users\\User\\Desktop\\parquet_files\\yellow_tripdata_2020-04.parquet'\n",
    "p2020_05_file = r'C:\\Users\\User\\Desktop\\parquet_files\\yellow_tripdata_2020-05.parquet'\n",
    "\n",
    "#jóval covid előtti\n",
    "data_2019_01 = pd.read_parquet(p2019_01_file, engine='auto')\n",
    "data_2019_02 = pd.read_parquet(p2019_02_file, engine='auto')\n",
    "\n",
    "#covid pánik\n",
    "data_2020_03 = pd.read_parquet(p2020_03_file, engine='auto')\n",
    "data_2020_04 = pd.read_parquet(p2020_04_file, engine='auto')\n",
    "data_2020_05 = pd.read_parquet(p2020_05_file, engine='auto')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_ = dd.read_parquet(p2020_03_file, engine='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59597523",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9be83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80149dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dd_)): \n",
    "    dd_['Travel_time_in_secs'][i] = (dd_['tpep_dropoff_datetime'][i]-dd_['tpep_pickup_datetime'][i]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd_['tpep_dropoff_datetime'][0]-pd_['tpep_pickup_datetime'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35988137",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):   \n",
    "    data_2020_03['Travel_time_in_secs'][i] = (data_2020_03['tpep_dropoff_datetime'][i]-data_2020_03['tpep_pickup_datetime'][i]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcca56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "i = 0\n",
    "\n",
    "difference = (data_2020_03['tpep_dropoff_datetime'][i]-data_2020_03['tpep_pickup_datetime'][i]).total_seconds()\n",
    "print(difference)\n",
    "\n",
    "#vendor ID: szolgáltatók\n",
    "#utazás ideje mp-ben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "difference = data_2020_03['tpep_dropoff_datetime'][i]-data_2020_03['tpep_pickup_datetime'][i]\n",
    "\n",
    "seconds_in_day = 24 * 60 * 60\n",
    "\n",
    "divmod(difference.days * seconds_in_day + difference.seconds, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020 = [data_2020_03, data_2020_04]\n",
    "s = 0\n",
    "for d in data_2020:\n",
    "  s += len(d)\n",
    "\n",
    "data_2019 = data_2019_01[0:s]\n",
    "whole_data = [data_2019]\n",
    "whole_data.extend(data_2020)\n",
    "\n",
    "print(f'Size of 2019 dataset: {len(whole_data[0])}, size of 2020 dataset: {s}')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc380012",
   "metadata": {},
   "source": [
    "## Adatok betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9b7bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_percent = 65\n",
    "test_percent = 25\n",
    "\n",
    "print(\"Train and test dataloading started...\")\n",
    "train_dataloader, test_dataloader = make_dataloaders(whole_data,batch_size,train_percent,test_percent)\n",
    "\n",
    "train_size = len(train_dataloader.dataset)\n",
    "test_size = len(test_dataloader.dataset)\n",
    "\n",
    "print(f'Size of train dataset: {train_size} ,size of test dataset: {test_size} proportion goodness is {(train_percent/test_percent)/(train_size/test_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "val_data_2020 = [data_2020_03[int((test_percent + 1)/100*len(data_2020_03)):int((test_percent + 3)/100*len(data_2020_03))],\n",
    "                 data_2020_04[int((test_percent + 1)/100*len(data_2020_04)):int((test_percent + 3)/100*len(data_2020_04))],\n",
    "                 data_2020_05[int((test_percent + 1)/100*len(data_2020_05)):int((test_percent + 3)/100*len(data_2020_05))]]\n",
    "\"\"\"\n",
    "\n",
    "val_data_2020 = [data_2020_05]\n",
    "Sum = 0\n",
    "for d in val_data_2020:\n",
    "  Sum += len(d)\n",
    "\n",
    "#val_data_2019 = data_2019_01[int((test_percent + 1)/100*len(data_2019_01)):s+int((test_percent + 1.5)/100*len(data_2019_01))]\n",
    "\n",
    "val_data_2019 = data_2019_02[0:Sum]\n",
    "val_data = [val_data_2019]\n",
    "val_data.extend(val_data_2020)\n",
    "\n",
    "val_size_2019 = len(val_data_2019)\n",
    "val_size_2020 = Sum\n",
    "\n",
    "print(f'Size of 2019 dataset: {val_size_2019} ,size of 2020 dataset: {val_size_2020}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62043197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Val dataloading started...\")\n",
    "val_dataloader = make_val_dataloader(val_data)\n",
    "print(f'Size of val dataset: {len(val_dataloader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31213fe",
   "metadata": {},
   "source": [
    "## Modell osztályok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "input_size = train_dataloader.dataset.tensors[0].size(1)\n",
    "output_size = train_dataloader.dataset.tensors[1].size(1)\n",
    "\n",
    "class myLNN1(nn.Module):\n",
    "  def __init__(self,Nin,Nout):\n",
    "    super(myLNN1,self).__init__()\n",
    "\n",
    "    self.L1 = nn.Linear(Nin,128)\n",
    "    self.L2 = nn.Linear(128,64)\n",
    "    self.L3 = nn.Linear(64,Nout)\n",
    "  \n",
    "  def forward(self,x):\n",
    "\n",
    "    x = torch.relu(self.L1(x))\n",
    "    x = torch.relu(self.L2(x))\n",
    "    x = self.L3(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "model1 = myLNN1(input_size,output_size)\n",
    "\n",
    "class myLNN2(nn.Module):\n",
    "  def __init__(self,Nin,Nout):\n",
    "    super(myLNN2,self).__init__()\n",
    "\n",
    "    self.L1 = nn.Linear(Nin,50)\n",
    "    self.L2 = nn.Linear(50,Nout)\n",
    "  \n",
    "  def forward(self,x):\n",
    "\n",
    "    x = torch.relu(self.L1(x))\n",
    "    x = self.L2(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "model2 = myLNN2(input_size,output_size)\n",
    "\n",
    "class myLNN3(nn.Module):\n",
    "  def __init__(self,Nin,Nout):\n",
    "    super(myLNN3,self).__init__()\n",
    "\n",
    "    self.L1 = nn.Linear(Nin,20)\n",
    "    self.L2 = nn.Linear(20,20)\n",
    "    self.L3 = nn.Linear(20,Nout)\n",
    "  \n",
    "  def forward(self,x):\n",
    "\n",
    "    x = torch.relu(self.L1(x))\n",
    "    x = torch.relu(self.L2(x))\n",
    "    x = self.L3(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "model3 = myLNN3(input_size,output_size)\n",
    "\n",
    "class myLNN4(nn.Module):\n",
    "  def __init__(self,Nin,Nout):\n",
    "    super(myLNN4,self).__init__()\n",
    "\n",
    "    self.L1 = nn.Linear(Nin,128)\n",
    "    self.L2 = nn.Linear(128,240)\n",
    "    self.L3 = nn.Linear(240,Nout)\n",
    "  \n",
    "  def forward(self,x):\n",
    "\n",
    "    x = torch.relu(self.L1(x))\n",
    "    x = torch.relu(self.L2(x))\n",
    "    x = self.L3(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "model4 = myLNN4(input_size,output_size)\n",
    "\n",
    "class myLNN5(nn.Module):\n",
    "  def __init__(self,Nin,Nout):\n",
    "    super(myLNN5,self).__init__()\n",
    "\n",
    "    self.L1 = nn.Linear(Nin,200)\n",
    "    self.L2 = nn.Linear(200,Nout)\n",
    "  \n",
    "  def forward(self,x):\n",
    "\n",
    "    x = torch.relu(self.L1(x))\n",
    "    x = self.L2(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "model5 = myLNN5(input_size,output_size)\n",
    "\n",
    "class expert_system(nn.Module):\n",
    "  def __init__(self,models,output_size):\n",
    "    super(expert_system,self).__init__()\n",
    "\n",
    "    self.model_parts = models\n",
    "  \n",
    "  def forward(self,x):\n",
    "\n",
    "    outs = torch.zeros(x.size(0),output_size)\n",
    "    outs = outs.clone().detach().requires_grad_(False)\n",
    "    for i in range(len(self.model_parts)):\n",
    "      \n",
    "      outs += self.model_parts[i](x)\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1,model2,model3,model4,model5]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849a075",
   "metadata": {},
   "source": [
    "## Tanítás és validálás (Single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6110661",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochNum = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model1\n",
    "\n",
    "optimizer = myOptimizers(model,learning_rate,1)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochNum, 1e-4)\n",
    "for i in range(epochNum):\n",
    "    print(f'Epoch {i+1}')\n",
    "    train_loop(model,loss_fn,train_dataloader,optimizer,scheduler)\n",
    "    test_loop(model,test_dataloader, 100)\n",
    "\n",
    "print(\"Validation...\")\n",
    "test_loop(model,val_dataloader,100)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb3572",
   "metadata": {},
   "source": [
    "## Tanítás és validálás (Expert system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochNum = 5\n",
    "\n",
    "j = 0\n",
    "for model in models:\n",
    "    loss_fn = myLossFunctions(0)\n",
    "    optimizer = myOptimizers(model,learning_rate,0)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochNum, 1e-4)\n",
    "    j += 1\n",
    "    print(f'Model {j}')\n",
    "    for i in range(epochNum):\n",
    "        print(f'Epoch {i+1}')\n",
    "        train_loop(model,loss_fn,train_dataloader,optimizer,scheduler)\n",
    "        test_loop(model,test_dataloader, 100)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83610c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.pop(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation...\")\n",
    "exp_sys = expert_system(models,output_size)\n",
    "test_loop(model1,val_dataloader,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b12b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'myExpertNet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d209c",
   "metadata": {},
   "source": [
    "## Vizualizáció"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5aded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE DATA\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Specify the names of the columns to include in the DataFrame\n",
    "usecols = ['dropoff_x','dropoff_y','pickup_x','pickup_y','dropoff_hour','pickup_hour','passenger_count', 'tip_amount']\n",
    "\n",
    "# Read the data from the \"nyc_taxi_wide.parq\" file into a Dask DataFrame\n",
    "# This contained location datas before Covid\n",
    "df = dd.read_parquet('nyc_taxi_wide.parq')[usecols].persist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4cfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EARLY TEST OF THE VISUALIZATION OF THE DATA WITH PLOT\n",
    "\n",
    "from holoviews.element.tiles import StamenTerrain\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Define the x and y ranges for the map and sample a small fraction of rows\n",
    "x_range, y_range =(-8242000,-8210000), (4965000,4990000)\n",
    "samples = df.sample(frac=1e-4)\n",
    "\n",
    "# Create a StamenTerrain object and a Points object, and combine them\n",
    "tiles = StamenTerrain().redim.range(x=x_range, y=y_range)\n",
    "points = hv.Points(samples, ['dropoff_x', 'dropoff_y'])\n",
    "tiles * hv.Points(df.sample(frac=1e-2), ['dropoff_x', 'dropoff_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be85cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING A HEATMAP BASED ON DROPOFF LOCATIONS\n",
    "\n",
    "import datashader as ds\n",
    "import holoviews.operation.datashader as hd\n",
    "from datashader.colors import Hot\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.element.tiles import StamenTerrain\n",
    "from holoviews.element.tiles import EsriImagery\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Convert the Points object from the DataFrame into a shaded raster image\n",
    "shaded = hd.datashade(hv.Points(df, ['dropoff_x', 'dropoff_y']), cmap=Hot, aggregator=ds.count('tip_amount'))\n",
    "\n",
    "# With the dynspread function adjust the shaded image, and set the visual options for it\n",
    "hd.dynspread(shaded, threshold=0.5, max_px=4).opts(bgcolor='black', xaxis=None, yaxis=None, width=900, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE OUTLIERS FROM THE TIP AMOUNT COLUMN - PREPARING FOR DRAWING A MAP BASED ON TIP AMOUNT AVERAGES\n",
    "#\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# Dask requirement to process data\n",
    "data = df['tip_amount'].compute()\n",
    "\n",
    "# Calculate summary statistics (mean and standard deviation) for the \"tip_amount\" column\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "print(mean(data))\n",
    "print(std(data))\n",
    "\n",
    "# Calculate the lower and upper cutoffs for identifying outliers\n",
    "cut_off = data_std * 1.5\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "print(lower)\n",
    "print(upper)\n",
    "\n",
    "# Identify the outliers in the \"tip_amount\" column, based on the cutoffs\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "print(outliers)\n",
    "\n",
    "# Filter the DataFrame to exclude values in the \"tip_amount\" column that are greater than the upper cutoff\n",
    "df = df[(df['tip_amount'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ae821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION OF THE AVERAGE TIP AMOUNT\n",
    "\n",
    "import colorcet as cc\n",
    "from holoviews.operation.datashader import rasterize\n",
    "\n",
    "# Create a Points object from a DataFrame, using the dropoff_x and dropoff_y columns\n",
    "points = hv.Points(df, ['dropoff_x', 'dropoff_y'])\n",
    "# Define a dictionary of options for the rasterize function (use the \"bmy\" color map, and color bar at the bottom)\n",
    "ropts = dict(tools=['hover'], colorbar=True, colorbar_position='bottom', cmap=cc.bmy, cnorm='eq_hist')\n",
    "# Use the rasterize function, and set the options for it\n",
    "taxi_trips = rasterize(points, aggregator=ds.mean('tip_amount')).opts(**ropts)\n",
    "# Combine the Raster object with a map_tiles object\n",
    "map_tiles * taxi_trips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
